# ğŸš€ GuÃ­a RÃ¡pida de Inicio

## InstalaciÃ³n RÃ¡pida

```bash
# 1. Clonar/descargar el proyecto
cd incendios

# 2. Crear entorno virtual (opcional pero recomendado)
python -m venv venv
source venv/bin/activate  # Windows: venv\Scripts\activate

# 3. Instalar dependencias
pip install -r requirements.txt
```

## Uso RÃ¡pido

### OpciÃ³n 1: Dashboard (MÃ¡s FÃ¡cil) â­

```bash
streamlit run dashboard.py
```

Abre tu navegador en `http://localhost:8501` y:
1. Ve a la pestaÃ±a "ğŸ“Š Datos y ETL"
2. Haz clic en "ğŸ”„ Generar/Cargar Datos"
3. Ve a "ğŸ¤– PredicciÃ³n de Riesgo"
4. Haz clic en "ğŸš€ Entrenar Modelo"
5. Haz clic en "ğŸ”® Generar PredicciÃ³n"
6. Ve a "ğŸ¯ OptimizaciÃ³n de Recursos"
7. Haz clic en "âš™ï¸ Optimizar AsignaciÃ³n"

### OpciÃ³n 2: Script de Ejemplo

```bash
python example_usage.py
```

Esto ejecuta todo el pipeline:
- Genera datos sintÃ©ticos
- Entrena modelo
- Genera predicciones
- Optimiza asignaciÃ³n de recursos

### OpciÃ³n 3: Script Principal

```bash
# Pipeline completo
python main.py --mode full

# Solo ETL
python main.py --mode etl

# Solo entrenar modelo
python main.py --mode train --model-type xgboost

# Solo predicciÃ³n
python main.py --mode predict

# Solo optimizaciÃ³n
python main.py --mode optimize --max-brigades 50
```

### OpciÃ³n 4: ProgramÃ¡tico

```python
from src.data.etl import FireDataETL
from src.models.prediction import FireRiskPredictor
from src.optimization.resource_allocation import ResourceAllocationOptimizer

# Ver ejemplo completo en example_usage.py
```

## Estructura de Datos

### Datos Procesados
Los datos procesados se guardan en:
- `data/processed/panel_incendios.parquet`

### Modelos Entrenados
Los modelos se guardan en:
- `models/fire_risk_model_*.pkl`

### Resultados
Los resultados se guardan en:
- `results/risk_map_*.csv` - Mapas de riesgo
- `results/allocation_*.csv` - AsignaciÃ³n de recursos

## ConfiguraciÃ³n

Edita `config.py` para ajustar:
- Tipo de modelo (xgboost, lightgbm, random_forest)
- NÃºmero mÃ¡ximo de brigadas
- NÃºmero mÃ¡ximo de bases
- Features del modelo
- ParÃ¡metros de optimizaciÃ³n

## Datos Reales

Por defecto, el sistema genera datos sintÃ©ticos. Para usar datos reales:

1. Descarga datasets de:
   - CONAF: https://www.conaf.cl/
   - CR2: http://www.cr2.cl/
   - NASA FIRMS: https://firms.modaps.eosdis.nasa.gov/

2. ColÃ³calos en `data/raw/`

3. Modifica `src/data/etl.py` para cargar tus archivos

## PrÃ³ximos Pasos

1. âœ… Instalar dependencias
2. âœ… Ejecutar dashboard o script de ejemplo
3. ğŸ“– Revisar notebooks en `notebooks/`
4. âš™ï¸ Ajustar configuraciÃ³n en `config.py`
5. ğŸ”„ Integrar datos reales
6. ğŸ¯ Personalizar modelos y features

## SoluciÃ³n de Problemas

### Error: "ModuleNotFoundError"
```bash
pip install -r requirements.txt
```

### Error: "No module named 'streamlit_folium'"
```bash
pip install folium streamlit-folium
```

### Error: "Solver not found" (optimizaciÃ³n)
Instala un solver de optimizaciÃ³n:
- Windows/Mac: `pip install pulp` (incluye CBC)
- Linux: `sudo apt-get install coinor-cbc`

### Datos muy lentos
- Usa menos aÃ±os de datos histÃ³ricos
- Reduce nÃºmero de comunas
- Usa `temporal_split=True` para entrenamiento mÃ¡s rÃ¡pido

## Recursos Adicionales

- ğŸ“– **README.md**: DocumentaciÃ³n completa
- ğŸ““ **notebooks/**: Ejemplos detallados
- ğŸ’» **example_usage.py**: CÃ³digo de ejemplo
- âš™ï¸ **config.py**: ConfiguraciÃ³n del proyecto

---

**Â¿Preguntas?** Abre un issue en el repositorio.

